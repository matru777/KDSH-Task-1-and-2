{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv langchain pypdf transformers pymupdf requests beautifulsoup4 PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from groq import Groq\n",
    "import statistics\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Groq client\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "if api_key is None:\n",
    "    raise ValueError(\"GROQ_API_KEY environment variable is not set\")\n",
    "client = Groq(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "# text = extract_text_from_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title_from_text(text):\n",
    "    parts = text.split(\"Abstract\")\n",
    "    \n",
    "    # The title is the content before \"Abstract\"\n",
    "    title = parts[0].strip()\n",
    "    return title\n",
    "# title = extract_title_from_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_and_extract_headings(file_path):\n",
    "    from langchain.prompts import PromptTemplate\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "    # Step 1: Load PDF\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Step 2: Split Text into Chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Step 3: Extract and clean Headings using Custom API\n",
    "    def clean_headings(raw_headings):\n",
    "        prompt_template = \"\"\"\n",
    "        Given the following list of extracted headings, clean and rewrite them to ensure clarity and correctness:\n",
    "         - Remove unwanted characters such as square brackets ([ ]), backslashes (\\), newlines (\\n), and extra spaces (but retain spaces between words).\n",
    "         - Split any combined headings into separate headings if they represent distinct topics.\n",
    "         - Eliminate duplicate or repeated headings.\n",
    "         - Only return clear and valid headings, and ensure they are concise.\n",
    "         - remove general heading , or basic heading which does not make any sense in the research paper\n",
    "          Input Headings: {raw_headings}\n",
    "          Return Format:\n",
    "          [\"Heading 1\", \"Heading 2\", \"Heading 3\", ...]\n",
    "          Output should strictly be a clean list of valid headings without any extra formatting, body text, or explanations.\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate(input_variables=[\"raw_headings\"], template=prompt_template)\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt.format(raw_headings=raw_headings)}],\n",
    "            model=\"gemma2-9b-it\",\n",
    "        )\n",
    "        return chat_completion.choices[0].message.content.strip()\n",
    "\n",
    "    def extract_headings(chunks):\n",
    "        prompt_template = \"\"\"\n",
    "        Extract only the broad headings and subheadings from the following text that are typically found in research papers. \n",
    "        The headings should be large and general , not detailed.\n",
    "        Only return the headings, do not include body text or explanations. \n",
    "        Text: {text}\n",
    "        Conditions:\n",
    "        1. If none of these headings or similar ones are found, return a blank space (\" \").\n",
    "        2. important to note that return a heading of maximum 3 words \n",
    "        3. Two consecutive headings must be at least a paragraph apart.\n",
    "        4. Return the headings in the format \"heading\" only, without any extra characters or formatting.  \n",
    "           If no headings are present, return a blank space (\" \").\n",
    "        Common headings to look for: \n",
    "         Abstract,Introduction, Methodology, Results, Discussion, Conclusion, Literature Review, References, \n",
    "        Acknowledgments, Background, Scope, Objectives, Problem Statement, Hypothesis, Significance, \n",
    "        Data Collection, Analysis, Findings, Future Work, Recommendations, Limitations, Theoretical Framework, \n",
    "        Ethical Considerations, Appendix, Table of Contents, List of Figures, Index, Contributions\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate(input_variables=[\"text\"], template=prompt_template)\n",
    "        raw_headings = []\n",
    "        for chunk in chunks:\n",
    "            chat_completion = client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt.format(text=chunk.page_content)}],\n",
    "                model=\"gemma2-9b-it\",\n",
    "            )\n",
    "            raw_headings.append(chat_completion.choices[0].message.content.strip())\n",
    "        return clean_headings(\"\\n\".join(raw_headings))\n",
    "\n",
    "    cleaned_headings = extract_headings(chunks)\n",
    "\n",
    "    # Step 4: Format the cleaned headings into a list format\n",
    "    def format_headings_to_list(cleaned_headings):\n",
    "        cleaned_headings = cleaned_headings.strip(\"[]\").replace('\"', '').replace(\" \", \"\").split(\",\")\n",
    "        return [heading.strip() for heading in cleaned_headings if heading]\n",
    "\n",
    "    return format_headings_to_list(cleaned_headings)\n",
    "# formatted_headings_list = process_pdf_and_extract_headings(pdf_path)\n",
    "# print(\"Formatted Headings List:\", formatted_headings_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def split_by_two_capitals(word_list):\n",
    "    updated_list = []\n",
    "    for word in word_list:\n",
    "        updated_word = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', word)\n",
    "        updated_list.append(updated_word)\n",
    "    print(updated_list)\n",
    "# formatted_headings_list=split_by_two_capitals(formatted_headings_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_headings_with_text(headings, text):\n",
    "    pattern = '|'.join(re.escape(h) for h in headings)\n",
    "    matches = re.split(f'({pattern})', text)  \n",
    "\n",
    "    pairs = []\n",
    "    used_headings = set()  \n",
    "    current_heading = None\n",
    "    current_paragraph = []\n",
    "\n",
    "    for part in matches:\n",
    "        part = part.strip() \n",
    "\n",
    "        if part in headings and part not in used_headings:  \n",
    "            if current_heading:\n",
    "                paragraph = ' '.join(current_paragraph)\n",
    "                if len(paragraph) >= 100:  # Exclude paragraphs with fewer than 100 characters\n",
    "                    pairs.append((current_heading, paragraph))\n",
    "                    used_headings.add(current_heading)\n",
    "            current_heading = part  \n",
    "            current_paragraph = [] \n",
    "        elif part: \n",
    "            current_paragraph.append(part)  \n",
    "    \n",
    "    if current_heading and current_paragraph:\n",
    "        paragraph = ' '.join(current_paragraph)\n",
    "        if len(paragraph) >= 100:  # Exclude paragraphs with fewer than 100 characters\n",
    "            pairs.append((current_heading, paragraph))\n",
    "            used_headings.add(current_heading)\n",
    "\n",
    "    return pairs\n",
    "# paired_text = pair_headings_with_text(formatted_headings_list, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_math_expressions_in_paired_text(paired_text):\n",
    "    new_paired_text = []\n",
    "    \n",
    "    for heading, paragraph in paired_text:\n",
    "        prompt = f\"\"\"\n",
    "        text: {paragraph}\n",
    "        The text contains mathematical terms and expressions.   \n",
    "        For example:\n",
    "        Input: p1 = (a^3)*5 + 8\n",
    "        Output: p1 is equal to a raised to the power of 3, multiplied by 5, plus 8.\n",
    "        Please rewrite the text, converting any mathematical expressions into plain language. If there are no mathematical expressions, keep the text unchanged.\n",
    "        Output should be clean and should not include body text or explanations.\n",
    "        \"\"\"\n",
    "        response = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=\"gemma2-9b-it\",\n",
    "        )\n",
    "        rewritten_paragraph = response.choices[0].message.content.strip()\n",
    "        new_paired_text.append((heading, rewritten_paragraph))\n",
    "    return new_paired_text\n",
    "# paired_text = rewrite_math_expressions_in_paired_text(paired_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_paragraph_for_heading(paired_text, target_heading):\n",
    "    for heading, paragraph in paired_text:\n",
    "        if heading.lower() == target_heading.lower(): \n",
    "            return paragraph\n",
    "    return None  \n",
    "# abstract = find_paragraph_for_heading(paired_text, \"Abstract\")\n",
    "# introduction = find_paragraph_for_heading(paired_text, \"Introduction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def evaluate_title_against_abstract(title, abstract):\n",
    "    prompt = f\"\"\"\n",
    "    Title: {title}\n",
    "    Abstract: {abstract}\n",
    "    Evaluate if the Title accurately reflects the main claims and content of the Abstract. Check for alignment in terms of key topics, scope, and focus.\n",
    "    Provide a score between 0 and 100, where 100 indicates perfect alignment.\n",
    "    Return only the score as an integer.\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        model=\"gemma2-9b-it\",\n",
    "    )\n",
    "\n",
    "    score = int(response.choices[0].message.content.strip())\n",
    "    return score\n",
    "# title_alignment_score = evaluate_title_against_abstract(title, abstract)\n",
    "\n",
    "# title_alignment_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_paragraph_flow(paired_text):\n",
    "    paragraph_scores = []\n",
    "    for i in range(len(paired_text) - 1):\n",
    "        prompt = f\"\"\"\n",
    "        Paragraph 1: {paired_text[i][1]}\n",
    "        Paragraph 2: {paired_text[i+1][1]}\n",
    "        Evaluate the logical flow between these two paragraphs. Assess if the explanation is clear, and the overall paper flow is maintained. Give a score between 0 and 100, where 100 represents perfect flow and 0 represents no logical connection.\n",
    "        Return only the score as an integer.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=\"gemma2-9b-it\",\n",
    "        )\n",
    "        score = int(response.choices[0].message.content.strip())\n",
    "        paragraph_scores.append(score)\n",
    "    if(paragraph_scores==[]):\n",
    "      return\n",
    "    return statistics.mean(paragraph_scores)\n",
    "# scores = evaluate_paragraph_flow(paired_text)\n",
    "# print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def evaluate_claims_against_results(paired_text):\n",
    "    sections = {heading.lower(): paragraph for heading, paragraph in paired_text}\n",
    "    if 'results' not in sections:\n",
    "        return \n",
    "    if 'abstract' not in sections and 'introduction' not in sections:\n",
    "        return  \n",
    "    combined_text = \"\"\n",
    "    if 'abstract' in sections and 'introduction' in sections:\n",
    "        combined_text = sections['abstract'] + \"\\n\\n\" + sections['introduction']\n",
    "    elif 'abstract' in sections:\n",
    "        combined_text = sections['abstract']\n",
    "    elif 'introduction' in sections:\n",
    "        combined_text = sections['introduction']\n",
    "    prompt = f\"\"\"\n",
    "    abstract/introduction: {combined_text}\n",
    "    Results: {sections['results']}\n",
    "    Only use this , if the Results part actually contain results of the research paper , otherwise if it contain other thing , return \"None\"\n",
    "    Evaluate the claims made in the abstract/introduction against the results. Check if the claims are clearly validated by the results.\n",
    "    Provide a score between 0 and 100, where 100 indicates perfect alignment.\n",
    "    Return only the score as an integer.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        model=\"gemma2-9b-it\",\n",
    "    )\n",
    "    if(response==\"None\" or \"none\"):\n",
    "        return \n",
    "    score = int(response.choices[0].message.content.strip())\n",
    "    return score\n",
    "# consistency_score = evaluate_claims_against_results(paired_text)\n",
    "# consistency_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_conclusion_clarity(paired_text):\n",
    "    sections = {heading.lower(): paragraph for heading, paragraph in paired_text}\n",
    "    if 'conclusion' not in sections:\n",
    "        return \n",
    "    prompt = f\"\"\"\n",
    "    Conclusion: {sections['conclusion']}\n",
    "    Only use this , if the conclusion part actually contain conclusion for the research paper , otherwise if it contain other thing , return \"None\"\n",
    "    Evaluate the clarity and accuracy of the conclusion. Check if the conclusion is concise, up to the point, and does not attempt to obscure or hide the outcome of the paper.\n",
    "    Provide a score between 0 and 100, where 100 indicates perfect clarity and transparency.\n",
    "    Return only the score as an integer \n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        model=\"gemma2-9b-it\",\n",
    "    )\n",
    "    score = int(response.choices[0].message.content.strip())\n",
    "    if(response==\"None\" or \"none\"):\n",
    "        return \n",
    "    return score\n",
    "# conclusion_score = evaluate_conclusion_clarity(paired_text)\n",
    "# print(conclusion_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_paragraph_grammar(paired_text):\n",
    "    paragraph_scores = []\n",
    "    \n",
    "    for i in range(len(paired_text)):\n",
    "        text = paired_text[i][1]\n",
    "        \n",
    "        # The prompt evaluates the fluency (grammar and coherence) of the text.\n",
    "        prompt = f\"\"\"\n",
    "        Paragraph: {text}\n",
    "        Evaluate the fluency of the English, which includes grammar, coherence, and clarity.\n",
    "        The fluency should be rated between 0 and 100, where 100 represents perfect fluency and 0 represents no coherence or clarity.\n",
    "        Return only the score as an integer and should not include body text or explanations.\n",
    "        \"\"\"\n",
    "        response = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=\"gemma2-9b-it\",\n",
    "        )\n",
    "        result = response.choices[0].message.content.strip()\n",
    "        try:\n",
    "            fluency_score = int(result)\n",
    "        except ValueError:\n",
    "            fluency_score = 0 \n",
    "        \n",
    "        paragraph_scores.append(fluency_score)\n",
    "    if(paragraph_scores==[]):\n",
    "        return\n",
    "    return statistics.mean(paragraph_scores)\n",
    "# scores_grammar = evaluate_paragraph_grammar(paired_text)\n",
    "# print(scores_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_paragraph_spelling(paired_text):\n",
    "    paragraph_scores = []\n",
    "    for i in range(len(paired_text)):\n",
    "        text = paired_text[i][1]\n",
    "        prompt = f\"\"\"\n",
    "        Paragraph: {text}\n",
    "        Count the spelling mistakes in the text (ignore mathematical expressions like 'x^2 + y = 10').\n",
    "        Return only the score as an integer and should not include body text or explanations.\n",
    "        Do not consider mathematical part during testing .\n",
    "        \"\"\"\n",
    "        response = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=\"gemma2-9b-it\",\n",
    "        )\n",
    "        result = response.choices[0].message.content.strip()\n",
    "        try:\n",
    "            num_spelling_mistakes = int(result)\n",
    "        except ValueError:\n",
    "            num_spelling_mistakes = 0  # Default in case of parsing error\n",
    "        \n",
    "        paragraph_scores.append(num_spelling_mistakes)\n",
    "    \n",
    "    return statistics.mean(paragraph_scores)\n",
    "# scores_spelling = evaluate_paragraph_spelling(paired_text)\n",
    "# print(scores_spelling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_google(query, domain_filter=None):\n",
    "    api_key = \"6780193db0bfc6546d1e2c15\"\n",
    "    url = \"https://api.scrapingdog.com/google\"\n",
    "    if domain_filter:\n",
    "        query += f\" {domain_filter}\"\n",
    "    params = {\n",
    "        \"api_key\": api_key,\n",
    "        \"query\": query,\n",
    "        \"results\": 10,\n",
    "        \"country\": \"us\",\n",
    "        \"page\": 0,\n",
    "        \"advance_search\": \"false\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException:\n",
    "        return None\n",
    "\n",
    "def get_arxiv_links_from_search(query, max_results=20, sort_by_date=False):\n",
    "    search_results = search_google(query, domain_filter=\"site:arxiv.org\")\n",
    "    links = []\n",
    "    if search_results and \"organic_results\" in search_results:\n",
    "        for result in search_results[\"organic_results\"][:max_results]:\n",
    "            link = result.get(\"link\", \"\")\n",
    "            if \"arxiv.org\" in link:\n",
    "                if \"/pdf/\" in link:\n",
    "                    link = link.replace(\"/pdf/\", \"/abs/\").split(\".pdf\")[0]\n",
    "                if \"/html/\" in link:\n",
    "                    link = link.replace(\"/html/\", \"/abs/\").split(\".html\")[0]\n",
    "                links.append(link)\n",
    "\n",
    "    links = list(dict.fromkeys(links))\n",
    "\n",
    "    if sort_by_date:\n",
    "        try:\n",
    "            links.sort(key=lambda link: link.split('/abs/')[-1][:4])\n",
    "        except Exception:\n",
    "            pass\n",
    "    return links\n",
    "\n",
    "def get_best_abstract_date(paper_title, original_abstract, max_retries=3, retry_delay=2, sort_by_date=False):\n",
    "    client = groq.Client(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "    def scrape_website(url, retries=0):\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            return soup.get_text()\n",
    "        except requests.exceptions.RequestException:\n",
    "            if retries < max_retries:\n",
    "                time.sleep(retry_delay)\n",
    "                return scrape_website(url, retries + 1)\n",
    "            return \"\"\n",
    "\n",
    "    def extract_info_from_text(original_abstract, scraped_text):\n",
    "        if not scraped_text.strip():\n",
    "            return \"January 14, 2025\"\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        Original Abstract: {original_abstract}\n",
    "        Scraped Text from website: {scraped_text}\n",
    "\n",
    "        1. Compare the scraped text with the original abstract.\n",
    "        2. If the abstract in the scraped text is similar to the original abstract, extract the publication date.\n",
    "        3. If no abstract is found or the scraped text does not match the original, return 'January 2025'.\n",
    "        4. If the abstract matches and a valid publication date is found, return the date only.\n",
    "\n",
    "        Please return only the publication date.\n",
    "        \"\"\"\n",
    "        response = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=\"gemma2-9b-it\"\n",
    "        )\n",
    "\n",
    "        if response and response.choices:\n",
    "            return response.choices[0].message.content.strip()\n",
    "        return \"January 14, 2025\"\n",
    "\n",
    "    arxiv_links = get_arxiv_links_from_search(paper_title)\n",
    "\n",
    "    for link in arxiv_links:\n",
    "        scraped_text = scrape_website(link)\n",
    "        extracted_date = extract_info_from_text(original_abstract, scraped_text)\n",
    "        if extracted_date != \"January 14, 2025\":\n",
    "            return extracted_date\n",
    "    return \"January 14, 2025\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import arxiv\n",
    "from datetime import datetime\n",
    "import time\n",
    "import statistics\n",
    "import groq\n",
    "\n",
    "# Function to search for Google and filter for arXiv links\n",
    "def search_google(query, domain_filter=None):\n",
    "    api_key = \"6780193db0bfc6546d1e2c15\"\n",
    "    url = \"https://api.scrapingdog.com/google\"\n",
    "    if domain_filter:\n",
    "        query += f\" {domain_filter}\"\n",
    "    params = {\n",
    "        \"api_key\": api_key,\n",
    "        \"query\": query,\n",
    "        \"results\": 10,\n",
    "        \"country\": \"us\",\n",
    "        \"page\": 0,\n",
    "        \"advance_search\": \"false\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        # print(f\"Google search successful. Found {len(response.json().get('organic_results', []))} results.\")\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # print(f\"Search failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to get arXiv links from search results\n",
    "def get_arxiv_links_from_search(query, max_results=20):\n",
    "    # print(f\"Searching for arXiv papers on '{query}'\")\n",
    "    search_results = search_google(query, domain_filter=\"site:arxiv.org\")\n",
    "    links = []\n",
    "    if search_results and \"organic_results\" in search_results:\n",
    "        for result in search_results[\"organic_results\"][:max_results]:\n",
    "            link = result.get(\"link\", \"\")\n",
    "            if \"arxiv.org\" in link:\n",
    "                if \"/pdf/\" in link:\n",
    "                    link = link.replace(\"/pdf/\", \"/abs/\").split(\".pdf\")[0]  # Convert PDF to abs link\n",
    "                if \"/html/\" in link:\n",
    "                    link = link.replace(\"/html/\", \"/abs/\").split(\".html\")[0]  # Convert HTML to abs link\n",
    "                links.append(link)\n",
    "    # print(f\"Found {len(links)} arXiv links.\")\n",
    "    return list(set(links))\n",
    "\n",
    "# Function to filter arXiv links by publication date\n",
    "def filter_arxiv_links_with_date_difference(links, date_cutoff):\n",
    "    # print(f\"Filtering links by date cutoff: {date_cutoff}\")\n",
    "    try:\n",
    "        cutoff_date = datetime.strptime(date_cutoff, '%B %d, %Y').date()\n",
    "    except ValueError as e:\n",
    "        # print(f\"Invalid date format for cutoff date: {e}\")\n",
    "        return []\n",
    "\n",
    "    filtered_links = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            paper_id = link.split(\"/\")[-1]\n",
    "            year_month = paper_id.split(\".\")[0]\n",
    "            year = int(\"20\" + year_month[:2])\n",
    "            month = int(year_month[2:])\n",
    "            paper_date = datetime(year, month, 1).date()\n",
    "\n",
    "            # Ensure paper_date is at least 1 year older than cutoff_date\n",
    "            if (cutoff_date - paper_date).days >= 250:\n",
    "                filtered_links.append(link)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing date for link {link}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"{len(filtered_links)} links passed the date and 1-year difference filter.\")\n",
    "    return filtered_links\n",
    "\n",
    "# Function to fetch metadata from arXiv\n",
    "def fetch_arxiv_metadata(arxiv_links, max_retries=3, retry_delay=2):\n",
    "    # print(f\"Fetching metadata for {len(arxiv_links)} arXiv links.\")\n",
    "    paper_data = []\n",
    "    for link in arxiv_links:\n",
    "        paper_id = link.split(\"/\")[-1]  # Extract the paper ID\n",
    "        success = False\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                search = arxiv.Search(id_list=[paper_id])\n",
    "                for result in arxiv.Client().results(search):\n",
    "                    paper_data.append({\n",
    "                        \"title\": result.title,\n",
    "                        \"abstract\": result.summary,\n",
    "                        \"published\": result.published.strftime('%B %d, %Y'),  # Ensure consistent date format\n",
    "                        \"link\": link,\n",
    "                    })\n",
    "                    success = True\n",
    "                    break\n",
    "                if success:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                # print(f\"Attempt {attempt + 1}/{max_retries} failed for {link}: {e}\")\n",
    "                time.sleep(retry_delay)\n",
    "\n",
    "        if not success:\n",
    "            print(f\"Failed to fetch metadata for {link} after {max_retries} attempts.\")\n",
    "            paper_data.append({\n",
    "                \"title\": None,\n",
    "                \"abstract\": None,\n",
    "                \"published\": None,\n",
    "                \"link\": link,\n",
    "                \"error\": f\"Failed after {max_retries} attempts.\"\n",
    "            })\n",
    "    print(f\"Fetched metadata for {len(paper_data)} papers.\")\n",
    "    return paper_data\n",
    "def extract_topics_using_llm(abstract, title ,introduction):\n",
    "    client = groq.Client(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "    prompt = f\"\"\"Analyze the given title and abstract and introduction of a research paper. Based on the key research topics, generate 4-5 alternative titles for the paper. Each title should be concise, consisting of 3-4 words, and should not be an exact copy of the original title. These new titles should reflect the major areas of focus in the paper and serve as helpful keywords for related studies.\n",
    "            Title: {title}\n",
    "            Abstract:{abstract}\n",
    "            Introduction:{introduction}\n",
    "            do not consider very general topic or any common words in the outut titles . \n",
    "            note then , output should be in the following format [title1,title2,title3] , do not return any explanation or body text , return list of titles only\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}], \n",
    "        model=\"gemma2-9b-it\",\n",
    "        temperature=0\n",
    "    )\n",
    "    if response and response.choices:\n",
    "        try:\n",
    "            topics = response.choices[0].message.content.strip()\n",
    "            return [topic.strip() for topic in topics.split(\",\")]\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to parse topics: {e}\")\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "# Function to summarize the title, abstract, and introduction\n",
    "def summarize_paper_content(title, abstract, introduction):\n",
    "    client = groq.Client(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "    prompt = f\"\"\"You are given the title, abstract, and introduction of a research paper. Summarize the key points into a concise paragraph, preserving the major concepts, research question, methods, and findings, while removing redundant information. The summarized paragraph should combine elements from the title, abstract, and introduction without losing the essence of the research.\n",
    "            Title: {title}\n",
    "            Abstract: {abstract}\n",
    "            Introduction: {introduction}\n",
    "            Please return a well-structured paragraph summary that combines the key aspects of the paper, which can be used as the input for further analysis.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}], \n",
    "        model=\"gemma2-9b-it\",\n",
    "        temperature=0\n",
    "    )\n",
    "    if response and response.choices:\n",
    "        try:\n",
    "            summarized_paragraph = response.choices[0].message.content.strip()\n",
    "            return summarized_paragraph\n",
    "        except Exception as e:\n",
    "            # print(f\"Failed to summarize content: {e}\")\n",
    "            return \"\"\n",
    "    return \"\"\n",
    "# Function to compute novelty scores using Groq's LLM\n",
    "def compute_novelty_scores_for_new_paper(input_abstract, existing_papers_abstracts):\n",
    "    # print(f\"Computing novelty scores for the new paper's abstract.\")\n",
    "    client = groq.Client(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "    scores = []\n",
    "\n",
    "    for paper_abstract in existing_papers_abstracts:\n",
    "        prompt = f\"\"\"Given two abstracts, evaluate the similarity between the new paper and the existing one. Rate the similarity on a scale of 0 to 100, where 0 indicates no similarity at all, and 100 indicates they are exactly the same.\\n\\nExisting Paper: {paper_abstract}\\nNew Paper: {input_abstract}\\nReturn only the score as an integer.\"\"\"\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}], \n",
    "            model=\"gemma2-9b-it\",\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        if response and response.choices:\n",
    "            try:\n",
    "                score = int(response.choices[0].message.content.strip())\n",
    "                scores.append(score)\n",
    "            except ValueError:\n",
    "                scores.append(0)\n",
    "        else:\n",
    "            scores.append(0)\n",
    "    # print(f\"Novelty scores computed: {scores}\")\n",
    "    return scores\n",
    "\n",
    "# Main process\n",
    "def main(title, abstract, introduction, date_cutoff):\n",
    "    try:\n",
    "        date_cutoff = datetime.strptime(date_cutoff, '%B %d, %Y').strftime('%B %d, %Y')\n",
    "    except ValueError as e:\n",
    "        # print(f\"Error in date format: {e}\")\n",
    "        return \n",
    "    summarized_abstract = summarize_paper_content(title, abstract, introduction)\n",
    "\n",
    "\n",
    "    topics = extract_topics_using_llm(summarized_abstract, title, introduction)\n",
    "\n",
    "    all_arxiv_links = []\n",
    "    for topic in topics:\n",
    "        links = get_arxiv_links_from_search(topic)\n",
    "        all_arxiv_links.extend(links)\n",
    "\n",
    "    all_arxiv_links = list(set(all_arxiv_links))\n",
    "    # print(f\"Total unique arXiv links collected: {len(all_arxiv_links)}\")\n",
    "\n",
    "    filtered_links = filter_arxiv_links_with_date_difference(all_arxiv_links, date_cutoff)\n",
    "\n",
    "    paper_data = fetch_arxiv_metadata(filtered_links)\n",
    "\n",
    "    existing_papers_abstracts = [paper[\"abstract\"] for paper in paper_data if paper[\"abstract\"]]\n",
    "    novelty_scores = compute_novelty_scores_for_new_paper(summarized_abstract, existing_papers_abstracts)\n",
    "    top_5_novelty_scores = sorted(novelty_scores, reverse=True)[:5]\n",
    "    novelty_score = statistics.mean(top_5_novelty_scores) if top_5_novelty_scores else 0\n",
    "    # print(f\"Average Novelty Score: {novelty_score}\")\n",
    "    return novelty_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to find threshold value for each checkpoint for testing pdf's using the given reference pdf**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(pdf_path):\n",
    "    # Dictionary to store results\n",
    "    results = {}\n",
    "    text=extract_text_from_pdf(pdf_path)\n",
    "    title = extract_title_from_text(text)\n",
    "    print(\"done_1\")\n",
    "    # Extract headings\n",
    "    formatted_headings_list = process_pdf_and_extract_headings(pdf_path)\n",
    "    print(\"done_2\")\n",
    "    # Format headings\n",
    "    formatted_headings_list = split_by_two_capitals(formatted_headings_list)\n",
    "    print(\"done_3\")\n",
    "    # Pair headings with text\n",
    "    paired_text = pair_headings_with_text(formatted_headings_list, text)\n",
    "    print(\"done_4\")\n",
    "    # Rewrite math expressions\n",
    "    paired_text = rewrite_math_expressions_in_paired_text(paired_text)\n",
    "    print(\"done_5\")\n",
    "    # Extract key sections\n",
    "    abstract = find_paragraph_for_heading(paired_text, \"Abstract\")\n",
    "    introduction = find_paragraph_for_heading(paired_text, \"Introduction\")\n",
    "    print(\"done_6\")\n",
    "    # Evaluate various aspects\n",
    "    results['evaluate_title_against_abstract'] = evaluate_title_against_abstract(title, abstract)\n",
    "    results['evaluate_conclusion_clarity'] = evaluate_conclusion_clarity(paired_text)\n",
    "    print(\"done_7\")\n",
    "    results['evaluate_paragraph_flow'] = evaluate_paragraph_flow(paired_text)\n",
    "    results['evaluate_claims_against_results'] = evaluate_claims_against_results(paired_text)\n",
    "    print(\"done_8\")\n",
    "    results['evaluate_paragraph_grammar'] = evaluate_paragraph_grammar(paired_text)\n",
    "    results['evaluate_paragraph_spelling'] = evaluate_paragraph_spelling(paired_text)\n",
    "    print(\"done_9\")\n",
    "    # Get best abstract date\n",
    "    date = get_best_abstract_date(title, abstract)\n",
    "    print(\"done_10\")\n",
    "    # Compute final score\n",
    "    results['Novalty_score'] = main(title, abstract, introduction, date)\n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "results = process_pdf(r\"enter the path of the pdf file\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Function to detect the publishable and non publishable using research paper***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_with_threshold(pdf_path):\n",
    "    try:\n",
    "        # Extract text from PDF\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "        if not text:\n",
    "            return 0\n",
    "\n",
    "        title = extract_title_from_text(text)\n",
    "        print(\"done_1\")\n",
    "        if not title:\n",
    "            return 0\n",
    "\n",
    "        # Extract headings\n",
    "        formatted_headings_list = process_pdf_and_extract_headings(pdf_path)\n",
    "        print(\"done_2\")\n",
    "        if not formatted_headings_list:\n",
    "            return 0\n",
    "\n",
    "        # Format headings\n",
    "        formatted_headings_list = split_by_two_capitals(formatted_headings_list)\n",
    "        print(\"done_3\")\n",
    "        if not formatted_headings_list:\n",
    "            return 0\n",
    "\n",
    "        # Pair headings with text\n",
    "        paired_text = pair_headings_with_text(formatted_headings_list, text)\n",
    "        print(\"done_4\")\n",
    "        if not paired_text:\n",
    "            return 0\n",
    "\n",
    "        # Rewrite math expressions\n",
    "        paired_text = rewrite_math_expressions_in_paired_text(paired_text)\n",
    "        print(\"done_5\")\n",
    "        if not paired_text:\n",
    "            return 0\n",
    "\n",
    "        # Extract key sections\n",
    "        abstract = find_paragraph_for_heading(paired_text, \"Abstract\")\n",
    "        introduction = find_paragraph_for_heading(paired_text, \"Introduction\")\n",
    "        print(\"done_6\")\n",
    "        if not abstract or not introduction:\n",
    "            return 0\n",
    "\n",
    "        # Evaluate various aspects\n",
    "        if evaluate_title_against_abstract(title, abstract) < 85:\n",
    "            return 0\n",
    "\n",
    "        if evaluate_conclusion_clarity(paired_text) < 75:\n",
    "            return 0\n",
    "\n",
    "        print(\"done_7\")\n",
    "        if evaluate_paragraph_flow(paired_text) < 70:\n",
    "            return 0\n",
    "\n",
    "        if evaluate_claims_against_results(paired_text) < 70:\n",
    "            return 0\n",
    "\n",
    "        print(\"done_8\")\n",
    "        if evaluate_paragraph_grammar(paired_text) < 75:\n",
    "            return 0\n",
    "\n",
    "        if evaluate_paragraph_spelling(paired_text) > 5:\n",
    "            return 0\n",
    "\n",
    "        print(\"done_9\")\n",
    "        # Get best abstract date\n",
    "        date = get_best_abstract_date(title, abstract)\n",
    "        print(\"done_10\")\n",
    "        if not date:\n",
    "            return 0\n",
    "\n",
    "        # Compute final score\n",
    "        if main(title, abstract, introduction, date) > 80:\n",
    "            return 0\n",
    "\n",
    "        return 1  # All checks passed\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF: {e}\")\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "input_directory = r\"enter the path of the directory containing PDF files\"\n",
    "output_csv = \"enter the path of the output CSV file\"\n",
    "\n",
    "results = []\n",
    "\n",
    "# Process each PDF in the directory\n",
    "for file_name in os.listdir(input_directory):\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(input_directory, file_name)\n",
    "        score = process_pdf_with_threshold(pdf_path)\n",
    "        results.append({\"PDF Name\": file_name, \"Score\": score})\n",
    "\n",
    "# Write results to CSV\n",
    "with open(output_csv, mode=\"w\", newline=\"\") as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=[\"PDF Name\", \"Score\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(results)\n",
    "\n",
    "print(f\"Scores written to {output_csv}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
